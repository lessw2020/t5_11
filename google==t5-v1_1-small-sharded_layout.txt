model = google/t5-v1_1-small, sharded with 2000000 parameters


--> google/t5-v1_1-small has 76.961152 Million params

model wrapping = 
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): T5ForConditionalGeneration(
      (shared): Embedding(32128, 512)
      (encoder): T5Stack(
        (embed_tokens): Embedding(32128, 512)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                  (relative_attention_bias): Embedding(32, 6)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (6): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (decoder): T5Stack(
        (embed_tokens): Embedding(32128, 512)
        (block): ModuleList(
          (0): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                  (relative_attention_bias): Embedding(32, 6)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (6): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (7): T5Block(
            (layer): ModuleList(
              (0): T5LayerSelfAttention(
                (SelfAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (1): T5LayerCrossAttention(
                (EncDecAttention): T5Attention(
                  (q): Linear(in_features=512, out_features=384, bias=False)
                  (k): Linear(in_features=512, out_features=384, bias=False)
                  (v): Linear(in_features=512, out_features=384, bias=False)
                  (o): Linear(in_features=384, out_features=512, bias=False)
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (2): T5LayerFF(
                (DenseReluDense): T5DenseGatedGeluDense(
                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)
                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)
                  (wo): Linear(in_features=1024, out_features=512, bias=False)
                  (dropout): Dropout(p=0.1, inplace=False)
                  (gelu_act): NewGELUActivation()
                )
                (layer_norm): T5LayerNorm()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (lm_head): Linear(in_features=512, out_features=32128, bias=False)
    )
  )
)


