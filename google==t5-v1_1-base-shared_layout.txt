model = google/t5-v1_1-base, sharded with 1000000 parameters


--> google/t5-v1_1-base has 247.577856 params

model wrapping = 
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): T5ForConditionalGeneration(
      (shared): Embedding(32128, 768)
      (encoder): T5Stack(
        (embed_tokens): Embedding(32128, 768)
        (block): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                      (relative_attention_bias): Embedding(32, 12)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (decoder): T5Stack(
        (embed_tokens): Embedding(32128, 768)
        (block): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                      (relative_attention_bias): Embedding(32, 12)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): T5Block(
                (layer): ModuleList(
                  (0): T5LayerSelfAttention(
                    (SelfAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (1): T5LayerCrossAttention(
                    (EncDecAttention): T5Attention(
                      (q): Linear(in_features=768, out_features=768, bias=False)
                      (k): Linear(in_features=768, out_features=768, bias=False)
                      (v): Linear(in_features=768, out_features=768, bias=False)
                      (o): Linear(in_features=768, out_features=768, bias=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                  (2): T5LayerFF(
                    (DenseReluDense): T5DenseGatedGeluDense(
                      (wi_0): Linear(in_features=768, out_features=2048, bias=False)
                      (wi_1): Linear(in_features=768, out_features=2048, bias=False)
                      (wo): Linear(in_features=2048, out_features=768, bias=False)
                      (dropout): Dropout(p=0.1, inplace=False)
                    )
                    (layer_norm): T5LayerNorm()
                    (dropout): Dropout(p=0.1, inplace=False)
                  )
                )
              )
            )
          )
        )
        (final_layer_norm): T5LayerNorm()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (lm_head): Linear(in_features=768, out_features=32128, bias=False)
    )
  )
)


